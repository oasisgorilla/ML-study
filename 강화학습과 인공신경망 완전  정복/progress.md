# 진행상황

## 10.10

#### 1. 왜 AI인가?

무어의 법칙으로 컴퓨팅 기술이 비약적으로 좋아지고 있음
<br>2016에는 AI가 최초로 이세돌 9단을 이긴 사례가 있음
<br>살짝 강의가 의심스럽긴 하지만 계속 들어보자.
<br>AI기술을 학습시키는데 게임이 많이 이용되고 있고
<br>AI기술을 사용해서 냉장창고의 전기 사용료를 40% 줄인 사례도 있음
<br> waitbutwhy.com 블로그를 추천해줌, 다양한 주제를 쉽고 재밌게 알려주는데, AI기술에 대한 글도 자주 씀

#### 2. 강의구조

#### 3. 강의자료

#### 4. chatGPT를 사용한 효율적으로 AI구축하기 유튜브 영상 추천

#### 5. 강화학습의 기본에 오신 것을 환영합니다.

#### 6. 공략계획

## 10.13

#### 7. 강화 학습이란 무엇인가?

특정 환경에서 에이전트(컴퓨터)가 행동을 통해 환경의 상태를 바꾸거나 보상을 받아서 최적의 행동을 찾아내는 과정
<br>로봇 개를 통해 사전 프로그래밍이 된 에이전트와 강화 학습이 된 에이전트와의 차이점 설명. <br>고정된 알고리즘과 강화 학습 알고리즘의 차이<br>
참고할만한 미디엄 블로그 링크와 논문을 제공함

#### 8. 벨만 방정식(Bellman Equation)

강화학습에서 사용되는 벨만 방정식에 대해 공부함<br>
[학습일지 링크](https://velog.io/@oasisgorilla/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EB%B2%A8%EB%A7%8C-%EB%B0%A9%EC%A0%95%EC%8B%9D)

#### 9. "계획"

앞서 배운 벨만 방정식을 바탕으로 에이전트가 어떻게 행동할지에 대한 "계획"을 세울 수 있음<br>
"정책"과는 다른 개념이니 이점 잘 숙지해두도록 한다.

## 10.15

#### 10. 마르코프 의사결정 과정(Markov Decision Process, MDP)

에이전트의 의사결정을 모델링할 수 있는 마르코프 의사결정 과정을 학습<br>
마르코프 의사결정 과정과 마르코프 과정의 차이, 결정론적 방법과 비결정론적 방법 학습

[학습일지 링크](https://velog.io/@oasisgorilla/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84-%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95-%EA%B3%BC%EC%A0%95Markov-Decision-Process)

## 10.16

#### 11. 정책 vs 계획
이제부터 보다 현실적인 비결정론적 탐색, 확률적인 탐색을 사용할 것이고,<br>
비결정론적 탐색에서의 임의적인 요소들 때문에 더이상 계획은 사용할 수 없다.<br>
계획은 다음 행동, 다음 단계를 정확히 알고 있을 때만 가능하기 때문이다.<br>
에이전트가 다양한 옵션에 따라서 경로를 설계하는 매커니즘 예시를 봤다.

[학습일지 링크](https://velog.io/@oasisgorilla/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%A0%95%EC%B1%85%EA%B3%BC-%EA%B3%84%ED%9A%8D)

#### 12. "리빙 패널티" 추가하기
리빙 패널티를 추가하여 에이전트가 최대한 게임을 빨리 끝내게 하는 요인을 제공할 수 있음<br>
리빙 패널티를 불구덩이에 들어가서 게임오버를 시켰을 때의 패널티보다 크게 설정했더니<br>
에이전트가 불구덩이에 들어가더라도 게임을 일찍 끝내는 결정을 하는 것은 인상적이었음<br>
에이전트는 정책에 따라서 인간보다 합리적인 결정을 내릴 수도 있다는 것을 보여주는 강의

